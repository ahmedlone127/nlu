{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_training_POS_demo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpTiuL1LRSxOkynfmMvnMQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zkufh760uvF3"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/JohnSnowLabs/nlu/blob/master/examples/collab/Training/NLU_training_demo.ipynb)\n","\n","\n","\n","# Training a Named Entity Recognition (POS) model with NLU \n","With the [POS tagger](https://nlp.johnsnowlabs.com/docs/en/annotators#postagger-part-of-speech-tagger) from Spark NLP you can achieve State Of the Art results on any POS problem.\n","It uses an Averaged Percetron Model approach under the hood.\n","\n","This notebook showcases the following features : \n","\n","- How to train the deep learning POS classifier\n","- How to store a pipeline to disk\n","- How to load the pipeline from disk (Enables NLU offline mode)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dur2drhW5Rvi"},"source":["# 1. Install Java 8 and NLU"]},{"cell_type":"code","metadata":{"id":"hFGnBCHavltY"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu  pyspark==2.4.7  > /dev/null \n","\n","import nlu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWp5LbydCkqC"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"f4KkTfnR5Ugg"},"source":["# 2. Download French POS dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrVb5ZMvvrQD","executionInfo":{"status":"ok","timestamp":1607912907225,"user_tz":-60,"elapsed":88820,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"9062f7af-9cfa-47c4-ec96-21b51e94f26d"},"source":["! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/fr/pos/UD_French/UD_French-GSD_2.3.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-12-14 02:28:26--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/fr/pos/UD_French/UD_French-GSD_2.3.txt\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.96.21\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.96.21|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3565213 (3.4M) [text/plain]\n","Saving to: ‘UD_French-GSD_2.3.txt’\n","\n","UD_French-GSD_2.3.t 100%[===================>]   3.40M  7.76MB/s    in 0.4s    \n","\n","2020-12-14 02:28:26 (7.76 MB/s) - ‘UD_French-GSD_2.3.txt’ saved [3565213/3565213]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0296Om2C5anY"},"source":["# 3. Train Deep Learning Classifier using nlu.load('train.pos')\n","\n","You dataset label column should be named 'y' and the feature column with text data should be named 'text'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"3ZIPkRkWftBG","executionInfo":{"status":"ok","timestamp":1607912968745,"user_tz":-60,"elapsed":150327,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"3b7db73c-d303-4bf8-e041-8d878de75be6"},"source":["import nlu\n","# load a trainable pipeline by specifying the train. prefix  and fit it on a datset with label and text columns\n","# Since there are no\n","train_path = '/content/UD_French-GSD_2.3.txt'\n","trainable_pipe = nlu.load('train.pos')\n","fitted_pipe = trainable_pipe.fit(dataset_path=train_path)\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict('Donald Trump and Angela Merkel dont share many oppinions')\n","preds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Donald</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Trump</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CCONJ</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Angela</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Merkel</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PRON</td>\n","      <td>dont</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>VERB</td>\n","      <td>share</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>ADJ</td>\n","      <td>many</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NOUN</td>\n","      <td>oppinions</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                pos      token\n","origin_index                  \n","0             PROPN     Donald\n","0             PROPN      Trump\n","0             CCONJ        and\n","0             PROPN     Angela\n","0             PROPN     Merkel\n","0              PRON       dont\n","0              VERB      share\n","0               ADJ       many\n","0              NOUN  oppinions"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"2BB-NwZUoHSe"},"source":["# 4. Lets save the model"]},{"cell_type":"code","metadata":{"id":"eLex095goHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607912971299,"user_tz":-60,"elapsed":152869,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"8d9c786d-dd0f-4e4b-ea20-bf9035d6c976"},"source":["stored_model_path = './models/pos_trained' \n","fitted_pipe.save(stored_model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Stored model in ./models/pos_trained\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e_b2DPd4rCiU"},"source":["# 5. Lets load the model from HDD.\n","This makes Offlien NLU usage possible!   \n","You need to call nlu.load(path=path_to_the_pipe) to load a model/pipeline from disk."]},{"cell_type":"code","metadata":{"id":"SO4uz45MoRgp","colab":{"base_uri":"https://localhost:8080/","height":500},"executionInfo":{"status":"ok","timestamp":1607912976676,"user_tz":-60,"elapsed":158238,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"3a3b8524-a01c-45e8-c0c2-936eced93711"},"source":["hdd_pipe = nlu.load(path=stored_model_path)\n","\n","preds = hdd_pipe.predict('Donald Trump and Angela Merkel dont share many oppinions on laws about cheeseburgers')\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting on empty Dataframe, could not infer correct training method!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Donald</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Trump</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CCONJ</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Angela</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PROPN</td>\n","      <td>Merkel</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PRON</td>\n","      <td>dont</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>VERB</td>\n","      <td>share</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>ADJ</td>\n","      <td>many</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NOUN</td>\n","      <td>oppinions</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>PRON</td>\n","      <td>on</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>VERB</td>\n","      <td>laws</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>ADV</td>\n","      <td>about</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NOUN</td>\n","      <td>cheeseburgers</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                pos          token\n","origin_index                      \n","0             PROPN         Donald\n","0             PROPN          Trump\n","0             CCONJ            and\n","0             PROPN         Angela\n","0             PROPN         Merkel\n","0              PRON           dont\n","0              VERB          share\n","0               ADJ           many\n","0              NOUN      oppinions\n","0              PRON             on\n","0              VERB           laws\n","0               ADV          about\n","0              NOUN  cheeseburgers"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"e0CVlkk9v6Qi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607912976678,"user_tz":-60,"elapsed":158233,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"ac359c58-1114-48e6-9c98-953dcabac659"},"source":["hdd_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])  | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)  | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)  | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)  | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)  | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['regex_tokenizer'] has settable params:\n","pipe['regex_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_tokenizer'].setTargetPattern('\\S+')  | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_tokenizer'].setMaxLength(99999)  | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['regex_tokenizer'].setMinLength(0)  | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['sentiment_dl'] has settable params:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o3jCHbIsMZrn"},"source":[""],"execution_count":null,"outputs":[]}]}